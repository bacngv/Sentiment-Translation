{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsCfCYtgMd-J",
        "outputId": "1ec2fcc4-02e6-46a0-b52a-e29882b4fe64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JFvvUEuWgHaX",
        "outputId": "631c16dd-5261-4ebc-8191-cf74f02a5daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vncorenlp in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2024.6.2)\n",
            "Requirement already satisfied: iteration_utilities in /usr/local/lib/python3.10/dist-packages (0.12.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install vncorenlp\n",
        "!pip install iteration_utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jhXyt0bxkVBq",
        "outputId": "3e6d50ef-fc79-4964-dd53-87b27cfa4839"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqNnxOvUo8aI",
        "outputId": "f10adc5d-222b-4382-81dc-2fdf258178e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.10/dist-packages (from pyvi) (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.10)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "import re\n",
        "from collections import Counter\n",
        "from vncorenlp import VnCoreNLP\n",
        "import torchtext\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "from torchtext import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "HunMOfrigWEY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "SEED = 2222\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Ghy2kxlJ7_",
        "outputId": "1faa0c9e-d2f4-4433-bc08-cca1f4955702"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e89dc9e8ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n",
        "spacy_en = spacy.load('en_core_web_sm')\n"
      ],
      "metadata": {
        "id": "rfH3saZtlJ-Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvi import ViTokenizer\n"
      ],
      "metadata": {
        "id": "GUWfMsC3oiA4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from iteration_utilities import deepflatten\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "ANsPHQ4coiDq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_en = 'Please put the dustpan in the broom closet'\n",
        "print(tokenize_en(text_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXNewMNioiGw",
        "outputId": "f2a27c2e-bdc4-4864-8365-18a1c9f8c4f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Please', 'put', 'the', 'dustpan', 'in', 'the', 'broom', 'closet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_vi(text):\n",
        "  tokens = ViTokenizer.tokenize(text).split()\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "L3ymrVEDlKDK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vi = 'cuốn sách này là của tôi. Của bạn đâu?'\n",
        "print(tokenize_vi(text_vi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FOjgSWJlKF7",
        "outputId": "19cea9eb-f3de-421c-e010-e4f41ad7e538"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cuốn', 'sách', 'này', 'là', 'của', 'tôi', '.', 'Của', 'bạn', 'đâu', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "target = Field(tokenize=tokenize_vi, init_token='<sos>', eos_token='<eos>', lower=True)"
      ],
      "metadata": {
        "id": "dZVsONLzpvWK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields = {\"English\": (\"src\", source), \"Vietnamese\": (\"trg\", target)}"
      ],
      "metadata": {
        "id": "-s29wuLup0G0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## split"
      ],
      "metadata": {
        "id": "B-cJjp4-qQJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_from_disk(\"/content/mt-en-vi\")\n",
        "train = pd.DataFrame(dataset['train'][:200000]).drop(columns=['source'])\n",
        "val = pd.DataFrame(dataset['validation']).drop(columns=['source'])\n",
        "test = pd.DataFrame(dataset['test']).drop(columns=['source'])\n",
        "\n",
        "train = train.rename(columns={'en': 'English', 'vi': 'Vietnamese'})\n",
        "val = val.rename(columns={'en': 'English', 'vi': 'Vietnamese'})\n",
        "test = test.rename(columns={'en': 'English', 'vi': 'Vietnamese'})\n"
      ],
      "metadata": {
        "id": "OcCHyryNLd3b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZOqPx8KVNpbN",
        "outputId": "83959269-24c7-40f7-d85b-160da700dfb9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             English  \\\n",
              "0              - Sorry, that question's not on here.   \n",
              "1         He wants you to come with him immediately.   \n",
              "2               I thought we could use some company.   \n",
              "3  It was founded in 2008 by this anonymous progr...   \n",
              "4  With both of these methods, no two prints are ...   \n",
              "\n",
              "                                          Vietnamese  \n",
              "0    - Xin lỗi, nhưng mà ở đây không có câu hỏi đấy.  \n",
              "1          Ông ấy muốn bố đi với ông ấy ngay lập tức  \n",
              "2  Tôi nghĩ chúng ta có thể muốn vài người bạn đồ...  \n",
              "3  Nó được sáng lập vào năm 2008 bởi một lập trìn...  \n",
              "4  Với cả hai phương pháp, không có hai bản in nà...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f027c0f1-2356-4c9e-86a5-29af57de8641\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Vietnamese</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>- Sorry, that question's not on here.</td>\n",
              "      <td>- Xin lỗi, nhưng mà ở đây không có câu hỏi đấy.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>He wants you to come with him immediately.</td>\n",
              "      <td>Ông ấy muốn bố đi với ông ấy ngay lập tức</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought we could use some company.</td>\n",
              "      <td>Tôi nghĩ chúng ta có thể muốn vài người bạn đồ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It was founded in 2008 by this anonymous progr...</td>\n",
              "      <td>Nó được sáng lập vào năm 2008 bởi một lập trìn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>With both of these methods, no two prints are ...</td>\n",
              "      <td>Với cả hai phương pháp, không có hai bản in nà...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f027c0f1-2356-4c9e-86a5-29af57de8641')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f027c0f1-2356-4c9e-86a5-29af57de8641 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f027c0f1-2356-4c9e-86a5-29af57de8641');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2a063836-2032-4718-9c5a-e92acbe6e9fd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a063836-2032-4718-9c5a-e92acbe6e9fd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2a063836-2032-4718-9c5a-e92acbe6e9fd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_json(\"train.json\", orient=\"records\", lines=True)\n",
        "test.to_json(\"test.json\", orient=\"records\", lines=True)\n",
        "val.to_json(\"val.json\", orient=\"records\", lines=True)"
      ],
      "metadata": {
        "id": "LoKTucNYgWKD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, val_data = TabularDataset.splits(\n",
        "    path=\"./\", train=\"train.json\", test=\"test.json\", validation =\"val.json\", format=\"json\", fields=fields\n",
        ")"
      ],
      "metadata": {
        "id": "ySx3yiuvqjEt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iiXwRFgqnwV",
        "outputId": "55f53c94-533a-40d0-feb0-60d766edae2f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsNrzUqrTF75",
        "outputId": "5728d418-02b0-4e51-80f7-f4ab431ba505"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11316"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MS5HVUNTIBX",
        "outputId": "b6d0a3cd-7be6-406c-d82d-880e47a40d61"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11225"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "for example in train_data.examples:  # chỉ lấy 5 mẫu đầu tiên\n",
        "    #print(example.trg)\n",
        "    cnt = cnt + 1\n",
        "\n",
        "print(cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-tzctPtuVoJ",
        "outputId": "9b40fb15-40e6-434e-b55d-cd40c318b239"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "target.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "metadata": {
        "id": "3shtC4ryrB_C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, val_data, test_data), batch_size=BATCH_SIZE, sort_key = lambda x: len(x.src),\n",
        "    sort_within_batch=True, device=device)\n",
        "\n",
        "test_batch = next(iter(test_iterator))\n",
        "test_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bwneAEYrCFe",
        "outputId": "eb19a897-d064-4ff6-cef2-b710e77954af"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 32]\n",
              "\t[.src]:[torch.cuda.LongTensor of size 6x32 (GPU 0)]\n",
              "\t[.trg]:[torch.cuda.LongTensor of size 10x32 (GPU 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adjustable parameters\n",
        "INPUT_DIM = len(source.vocab)\n",
        "OUTPUT_DIM = len(target.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ],
      "metadata": {
        "id": "L0dselmUq1S6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM, OUTPUT_DIM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fliy4cFPq1WN",
        "outputId": "51f7c168-2906-4cb6-aa8b-772b35be6adb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10004, 10004)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim: int, emb_dim: int, hid_dim: int, n_layers: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, src_batch: torch.LongTensor):\n",
        "        embedded = self.embedding(src_batch) # [sent len, batch size, emb dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "25dLc0mRs9_O"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "hidden, cell = encoder(test_batch.src)\n",
        "hidden.shape, cell.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO5qEPnks-CU",
        "outputId": "351a7bb6-332b-47c7-89d9-28c3bf7bdc1c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 32, 512]), torch.Size([2, 32, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim: int, emb_dim: int, hid_dim: int, n_layers: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, trg: torch.LongTensor, hidden: torch.FloatTensor, cell: torch.FloatTensor):\n",
        "        # [1, batch size, emb dim], the 1 serves as sent len\n",
        "        embedded = self.embedding(trg.unsqueeze(0))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.out(outputs.squeeze(0))\n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "4EhnvzMts-FK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "\n",
        "# notice that we are not passing the entire the .trg\n",
        "prediction, hidden, cell = decoder(test_batch.trg[0], hidden, cell)\n",
        "prediction.shape, hidden.shape, cell.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0aQqaT4tOyE",
        "outputId": "e88304ca-64aa-4a2f-b234-e171aef301ee"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 10004]), torch.Size([2, 32, 512]), torch.Size([2, 32, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            'Hidden dimensions of encoder and decoder must be equal!'\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            'Encoder and decoder must have equal number of layers!'\n",
        "\n",
        "    def forward(self, src_batch: torch.LongTensor, trg_batch: torch.LongTensor,\n",
        "                teacher_forcing_ratio: float=0.5):\n",
        "\n",
        "        max_len, batch_size = trg_batch.shape\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tensor to store decoder's output\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
        "        hidden, cell = self.encoder(src_batch)\n",
        "\n",
        "        trg = trg_batch[0]\n",
        "        for i in range(1, max_len):\n",
        "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
        "            outputs[i] = prediction\n",
        "\n",
        "            if random.random() < teacher_forcing_ratio:\n",
        "                trg = trg_batch[i]\n",
        "            else:\n",
        "                trg = prediction.argmax(1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "2B2EgJlzvFUB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
        "seq2seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHvc4qZkvFWs",
        "outputId": "ca144554-1517-492b-990c-a27c398f127d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(10004, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10004, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (out): Linear(in_features=512, out_features=10004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = seq2seq(test_batch.src, test_batch.trg)\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNeqYgzLtO3n",
        "outputId": "d869ba74-7d0e-4225-b7a1-51e0583dc169"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 32, 10004])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(seq2seq):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq1cvdWFtO9j",
        "outputId": "9b0abd51-09e0-496e-d68a-e6425d6ef9a2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 17,610,516 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(seq2seq.parameters())\n",
        "\n",
        "# ignore the padding index when calculating the loss\n",
        "PAD_IDX = target.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "6yuxAKjpv5sd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(seq2seq, iterator, optimizer, criterion):\n",
        "    seq2seq.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = seq2seq(batch.src, batch.trg)\n",
        "\n",
        "        # 1. as mentioned in the seq2seq section, we will\n",
        "        # cut off the first element when performing the evaluation\n",
        "        # 2. the loss function only works on 2d inputs\n",
        "        # with 1d targets we need to flatten each of them\n",
        "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
        "        trg_flatten = batch.trg[1:].view(-1)\n",
        "        loss = criterion(outputs_flatten, trg_flatten)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "YXjiUj6Pqn2A"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(seq2seq, iterator, criterion):\n",
        "    seq2seq.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            # turn off teacher forcing\n",
        "            outputs = seq2seq(batch.src, batch.trg, teacher_forcing_ratio=0)\n",
        "\n",
        "            # trg = [trg sent len, batch size]\n",
        "            # output = [trg sent len, batch size, output dim]\n",
        "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
        "            trg_flatten = batch.trg[1:].view(-1)\n",
        "            loss = criterion(outputs_flatten, trg_flatten)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "IB4ollWFwHxd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "oDkQnzFOwOI0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(seq2seq, train_iterator, optimizer, criterion)\n",
        "    valid_loss = evaluate(seq2seq, valid_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(seq2seq.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    # it's easier to see a change in perplexity between epoch as it's an exponential\n",
        "    # of the loss, hence the scale of the measure is much bigger\n",
        "    print(f'Epoch: {epoch+1} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWazyYRmwH0S",
        "outputId": "3c0c975e-21ba-4506-b08d-787f7e608b95"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Time: 11m 9s\n",
            "\tTrain Loss: 5.146 | Train PPL: 171.756\n",
            "\t Val. Loss: 5.083 |  Val. PPL: 161.287\n",
            "Epoch: 2 | Time: 10m 30s\n",
            "\tTrain Loss: 4.521 | Train PPL:  91.904\n",
            "\t Val. Loss: 4.806 |  Val. PPL: 122.191\n",
            "Epoch: 3 | Time: 10m 44s\n",
            "\tTrain Loss: 4.230 | Train PPL:  68.727\n",
            "\t Val. Loss: 4.646 |  Val. PPL: 104.124\n",
            "Epoch: 4 | Time: 10m 42s\n",
            "\tTrain Loss: 4.035 | Train PPL:  56.566\n",
            "\t Val. Loss: 4.582 |  Val. PPL:  97.714\n",
            "Epoch: 5 | Time: 10m 30s\n",
            "\tTrain Loss: 3.894 | Train PPL:  49.088\n",
            "\t Val. Loss: 4.521 |  Val. PPL:  91.907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "3b22pdNfwpyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(seq2seq, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-4GJOQzgLvS",
        "outputId": "b211e398-8ba0-4c7a-f1ed-22a5ab212427"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 4.523 | Test PPL:  92.085 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 100\n",
        "example = test_data.examples[example_idx]\n",
        "print('source sentence: ', ' '.join(example.src))\n",
        "print('target sentence: ', ' '.join(example.trg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTniTvkswwPq",
        "outputId": "6b6a65b7-d197-4754-9163-85bfc8c20b3a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source sentence:  men fall from the sky . and gods hurl thunderbolts .\n",
            "target sentence:  người rơi từ trên trời xuống , các vị thần phóng ra sét .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_tensor = source.process([example.src]).to(device)\n",
        "trg_tensor = target.process([example.trg]).to(device)\n",
        "print(trg_tensor.shape)\n",
        "\n",
        "seq2seq.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = seq2seq(src_tensor, trg_tensor, teacher_forcing_ratio=0.5)\n",
        "\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egt0BcSKw3LL",
        "outputId": "648bc28c-58f1-412a-b106-7d3ab5237093"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 1, 10004])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
        "' '.join([target.vocab.itos[idx] for idx in output_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "20f3c731w3OA",
        "outputId": "dca665ca-43b6-4b77-b8e5-73c77932cf13"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'người của từ từ , , và và <unk> thần <unk> <unk> . . <eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "from torchtext.data import Field\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you've defined your Encoder, Decoder, and Seq2Seq models already\n",
        "\n",
        "def predict_example(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input sentence\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = src_field.tokenize(sentence)\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Numericalize the tokens\n",
        "    numericalized_tokens = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    tensor = torch.LongTensor(numericalized_tokens).unsqueeze(1).to(device)  # shape: (src_len, 1)\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(tensor)\n",
        "\n",
        "    # Prepare the input to the decoder\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)  # shape: (1)\n",
        "\n",
        "        # Forward pass through decoder\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        # Get most likely word index from output\n",
        "        pred_token = output.argmax(1).item()\n",
        "\n",
        "        # Append prediction to current output prediction\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # Stop appending if we predicted the end of sentence token\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    # Convert numerical indices to tokens\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # Return the tokens, excluding the start of sequence token\n",
        "    return trg_tokens[1:]\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Load your trained model\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
        "seq2seq.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "# Define the source and target fields\n",
        "source = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "target = Field(tokenize=tokenize_vi, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "\n",
        "# Load vocabularies\n",
        "source.vocab = train_data.fields['src'].vocab\n",
        "target.vocab = train_data.fields['trg'].vocab"
      ],
      "metadata": {
        "id": "8MMIMT9Ci8rr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence to predict\n",
        "example_sentence = \"who am I?\"\n",
        "\n",
        "# Predict\n",
        "predicted_sentence = predict_example(example_sentence, source, target, seq2seq, device)\n",
        "print(\"Predicted:\", ' '.join(predicted_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dIx30k3l680",
        "outputId": "d043369d-ca91-4cc1-bf0f-fac27ce85ca2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ai là ai là ai ? <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence to predict\n",
        "example_sentence = \"what is your name?\"\n",
        "\n",
        "# Predict\n",
        "predicted_sentence = predict_example(example_sentence, source, target, seq2seq, device)\n",
        "print(\"Predicted:\", ' '.join(predicted_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z42gYHf-l-jD",
        "outputId": "1934bb0e-22cf-4436-dd7e-34ba9ef64702"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: tên của là gì là gì ? <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU"
      ],
      "metadata": {
        "id": "TlxpRUbr87Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkCE8OrZ8BAl",
        "outputId": "267c7974-3e64-49a2-8f43-4f6dbd15803f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "Q0DfCEao8BC_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "NZOqozUG-K8C"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "seq2seq.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss = evaluate(seq2seq, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
        "\n",
        "references = []\n",
        "candidates = []\n",
        "\n",
        "seq2seq.eval()\n",
        "with torch.no_grad():\n",
        "    for example in test_data.examples:\n",
        "        src_tensor = source.process([example.src]).to(device)\n",
        "        trg_tensor = target.process([example.trg]).to(device)\n",
        "\n",
        "        outputs = seq2seq(src_tensor, trg_tensor, teacher_forcing_ratio=0.5)\n",
        "        output_idx = outputs[1:].squeeze(1).argmax(1)\n",
        "        translated_sentence = [target.vocab.itos[idx] for idx in output_idx]\n",
        "\n",
        "        # Append reference and candidate\n",
        "        references.append([example.trg])  # Note that references are expected to be a list of lists\n",
        "        candidates.append(translated_sentence)\n",
        "\n",
        "# Calculate BLEU score for the entire test set\n",
        "smooth_fn = SmoothingFunction().method4  # Use smoothing to handle short sentences\n",
        "bleu_score = corpus_bleu(references, candidates, smoothing_function=smooth_fn)\n",
        "print(f'Corpus BLEU score: {bleu_score:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK4mugvU8BFl",
        "outputId": "aeff6c8e-664b-4c62-ef0c-2b6d4bf88478"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 4.523 | Test PPL:  92.085 |\n",
            "Corpus BLEU score: 0.047\n"
          ]
        }
      ]
    }
  ]
}